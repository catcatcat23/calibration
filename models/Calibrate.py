import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class Calibrate_Model(nn.Module):
    """
    calibrate_method:
      - 'ts'      : æ¸©åº¦ç¼©æ”¾ï¼Œforward è¿”å› logits/T
      - 'ets'     : ETS(ä¸‰ç»„ä»¶)ï¼Œforward è¿”å› log(q) ä½œä¸º "logits"
      - 'ets_pc'  : ETS + æŒ‰ç±»æ¸©åº¦ï¼ˆæ¯ç±»ä¸€ä¸ª Tï¼‰
    éæƒ°æ€§ï¼šé€‰æ‹© ETS æ—¶ç«‹å³åˆ›å»º Î± å‚æ•°ï¼ˆéœ€ä¼ å…¥ç±»åˆ«æ•°ï¼‰
    """
    def __init__(self, model, T=None):
        super().__init__()
        self.model = model
        self.T = nn.Parameter(torch.tensor(1.0 if T is None else T, dtype=torch.float32))

        # ç»Ÿä¸€æ¨¡å¼å¼€å…³ï¼ˆé»˜è®¤ TSï¼‰ï¼Œå¯¹å¤–ä¸€å¾‹â€œåƒ logitsâ€
        self.calib_method = 'ts'
        self._returns_log_prob = False  # æˆ‘ä»¬ç»Ÿä¸€æŠŠè¾“å‡ºå½“ logits ç”¨ï¼ˆETS è¿”å› log(q)ï¼‰

        # ===== ETS ç›¸å…³ï¼ˆéæƒ°æ€§ï¼‰=====
        self._etsm_enabled = False
        self._etsm_per_class = False
        self._etsm_eps = 1e-8
        self._etsm_w_logits = None
        self._etsm_alpha_raw_bos = None
        self._etsm_alpha_raw_bom = None
        self._etsm_init_alpha = 1.0
        # è®°å½•ç±»åˆ«æ•°ï¼Œä¾¿äºæ ¡éªŒ
        self._num_classes_bos = None
        self._num_classes_bom = None

    # ========= ä¸»å…¥å£ï¼šç”± args é©±åŠ¨ï¼Œéæƒ°æ€§åˆ›å»º Î± ========= ğŸ”§
    def set_calibrate_method(self, method: str,
                             per_class: bool = False,
                             init_T: float = 1.0,
                             num_classes_bom: int = None,
                             num_classes_bos: int = None):
        """
        å‚æ•°ï¼š
          method: 'ts' | 'ets' | 'ets_pc'
          per_class: ETS æ˜¯å¦æŒ‰ç±»æ¸©åº¦
          init_T: ETS åˆå§‹ Tï¼ˆÎ±=1/Tï¼‰
          num_classes_bom/bos: å¯¹åº”å¤´çš„ç±»åˆ«æ•°ï¼ˆETS ä¸‹å¿…éœ€ï¼›TS å¯å¿½ç•¥ï¼‰
        """
        method = (method or 'ts').lower()
        if method == 'ts':
            self.calib_method = 'ts'
            self._returns_log_prob = False
            self._etsm_enabled = False
            return

        # ETS / ETS_PC
        if num_classes_bom is None and num_classes_bos is None:
            raise ValueError("ETS éœ€è¦æä¾› num_classes_bom æˆ– num_classes_bosï¼ˆè‡³å°‘ä¸€ä¸ªå¤´çš„ Kï¼‰")

        self._num_classes_bos = num_classes_bos
        self._num_classes_bom = num_classes_bom
        self._etsm_per_class = bool(per_class or method in ('ets_pc','ets-pc'))
        self._etsm_eps = 1e-8
        self._etsm_enabled = True
        self._etsm_init_alpha = 1.0 / max(init_T, 1e-6)

        # ç«‹å³åˆ›å»ºå‚æ•°ï¼ˆéæƒ°æ€§ï¼‰ ğŸ”§
        self._create_ets_params()

        self.calib_method = 'ets_pc' if self._etsm_per_class else 'ets'
        self._returns_log_prob = False  # å¯¹å¤–ä»ç„¶å½“ä½œ logits ç”¨
    def _infer_device(self):
        # å–å·²æœ‰å‚æ•°çš„ deviceï¼›ä¼˜å…ˆéª¨å¹²ï¼Œå…¶æ¬¡ T
        for p in self.model.parameters():
            return p.device
        return self.T.device
    # ========= ç«‹å³åˆ›å»º ETS å‚æ•°ï¼ˆéæƒ°æ€§ï¼‰ ========= ğŸ”§
    def _create_ets_params(self):
        # 3 ä¸ªæ··åˆæƒé‡ï¼ˆsoftmax çº¦æŸï¼‰
        self._etsm_w_logits = nn.Parameter(torch.zeros(3, dtype=torch.float32))
        device = self._infer_device()
        init_raw = float(np.log(np.exp(self._etsm_init_alpha) - 1.0))  # Î± = softplus(raw)+eps
        # bos å¤´
        if self._num_classes_bos is not None:
            if self._etsm_per_class:
                self._etsm_alpha_raw_bos = nn.Parameter(torch.full(
                    (self._num_classes_bos,), init_raw, dtype=torch.float32, device=device))
            else:
                self._etsm_alpha_raw_bos = nn.Parameter(torch.tensor([init_raw], dtype=torch.float32, device=device))
        else:
            self._etsm_alpha_raw_bos = None

        # bom å¤´
        if self._num_classes_bom is not None:
            if self._etsm_per_class:
                self._etsm_alpha_raw_bom = nn.Parameter(torch.full(
                    (self._num_classes_bom,), init_raw, dtype=torch.float32,device=device))
            else:
                self._etsm_alpha_raw_bom = nn.Parameter(torch.tensor([init_raw], dtype=torch.float32,device=device))
        else:
            self._etsm_alpha_raw_bom = None

    # ========= forwardï¼šç»Ÿä¸€è°ƒç”¨ï¼Œä¸åšæƒ°æ€§åˆå§‹åŒ– =========
    def forward(self, img1, img2):
        if self.calib_method.startswith('ets') and self._etsm_enabled:
            return self._forward_ets_as_logits(img1, img2)

        # TSï¼šè¿”å› logits / T
        logits_bos, logits_bom = self.model(img1, img2)
        out_bos = None if logits_bos is None else logits_bos / self.T
        out_bom = None if logits_bom is None else logits_bom / self.T
        self._returns_log_prob = False
        return out_bos, out_bom

    def calibrate_test(self, img1, img2):
        self.eval()
        with torch.no_grad():
            return self(img1, img2)

    # ========= å†»ç»“éª¨å¹²ï¼Œåªå­¦æ ¡å‡†å‚æ•° =========
    def pre_fintune(self):
        for p in self.model.parameters():
            p.requires_grad = False

        if self.calib_method == 'ts':
            self.T.requires_grad = True
        else:
            # ETSï¼šw ä¸ Î± å¯è®­ç»ƒï¼ˆéæƒ°æ€§æ—¶å¿…å®šå·²å­˜åœ¨ï¼‰
            assert self._etsm_w_logits is not None, "ETS å‚æ•°æœªåˆå§‹åŒ–ï¼›è¯·å…ˆ set_calibrate_method(..., num_classes_*)"
            self._etsm_w_logits.requires_grad = True
            if self._etsm_alpha_raw_bos is not None:
                self._etsm_alpha_raw_bos.requires_grad = True
            if self._etsm_alpha_raw_bom is not None:
                self._etsm_alpha_raw_bom.requires_grad = True
            # å¦‚éœ€åŒæ—¶å­¦æ ¡å‡†å…¨å±€ Tï¼Œå¯æ‰“å¼€ï¼š
            # self.T.requires_grad = True

    # ========= å…¶ä»–å·¥å…·ï¼ˆä¿æŒä½ åŸæ¥çš„ï¼‰ =========
    def compile_model(self):
        self.model = torch.jit.script(self.model)

    def grid_search_set(self, T_range, dt):
        self.candidate_T_arange = np.arange(T_range[0], T_range[1] + dt, dt)
        self.T_index = 0

    def reflash_T(self):
        assert self.T_index < len(self.candidate_T_arange), "å·²è¶…è¿‡å¯é€‰TèŒƒå›´"
        self.T = nn.Parameter(torch.tensor(self.candidate_T_arange[self.T_index], dtype=torch.float32))

    def reset_index(self, T_idx=None):
        self.T_index = T_idx if T_idx is not None else (self.T_index + 1)

    def __len__(self):
        print(f"å¯é€‰TèŒƒå›´ä¸º: {len(self.candidate_T_arange)}")
        return len(self.candidate_T_arange)

    # ========= ETS è®¡ç®— =========
    def _etsm_softplus_pos(self, x):  # Î±>0
        return F.softplus(x) + self._etsm_eps

    def _etsm_ts_prob(self, p, alpha):
        # p^{alpha} / sum p^{alpha}ï¼Œalpha: æ ‡é‡æˆ– [K]
        if alpha.ndim == 0:
            a = alpha.view(1, 1)
        else:
            a = alpha.view(1, -1)
        p_pow = torch.clamp(p, min=self._etsm_eps) ** a
        return p_pow / p_pow.sum(dim=1, keepdim=True)

    def _forward_ets_as_logits(self, img1, img2):
        """
        ETSï¼š
          logits -> p -> q = w1*TS(p;Î±) + w2*p + w3*uniform
          è¿”å› log(q) ä½œä¸º "logits"ï¼ˆCE/Focal ç›´æ¥å¯ç”¨ï¼‰
        """
        assert self._etsm_enabled, "è¯·å…ˆ set_calibrate_method('ets'/'ets_pc', ...)"
        logits_bos, logits_bom = self.model(img1, img2)
        w = torch.softmax(self._etsm_w_logits, dim=0)  # [3]

        out_bos = None
        if logits_bos is not None:
            if self._num_classes_bos is not None:
                assert logits_bos.shape[1] == self._num_classes_bos, \
                    f"bos ç±»åˆ«æ•°ä¸ä¸€è‡´ï¼šlogits:{logits_bos.shape[1]} vs é…ç½®:{self._num_classes_bos}"
            if self._etsm_alpha_raw_bos is not None:
                alpha_bos = self._etsm_softplus_pos(self._etsm_alpha_raw_bos)
                p = F.softmax(logits_bos, dim=1)
                ts = self._etsm_ts_prob(p, alpha_bos)
                uni = torch.full_like(p, 1.0 / p.shape[1])
                q = w[0] * ts + w[1] * p + w[2] * uni
                out_bos = torch.log(q.clamp_min(self._etsm_eps))
            else:
                out_bos = None  # æœªé…ç½® bos

        out_bom = None
        if logits_bom is not None:
            if self._num_classes_bom is not None:
                assert logits_bom.shape[1] == self._num_classes_bom, \
                    f"bom ç±»åˆ«æ•°ä¸ä¸€è‡´ï¼šlogits:{logits_bom.shape[1]} vs é…ç½®:{self._num_classes_bom}"
            if self._etsm_alpha_raw_bom is not None:
                alpha_bom = self._etsm_softplus_pos(self._etsm_alpha_raw_bom)
                p = F.softmax(logits_bom, dim=1)
                ts = self._etsm_ts_prob(p, alpha_bom)
                uni = torch.full_like(p, 1.0 / p.shape[1])
                q = w[0] * ts + w[1] * p + w[2] * uni
                out_bom = torch.log(q.clamp_min(self._etsm_eps))
            else:
                out_bom = None  # æœªé…ç½® bom

        self._returns_log_prob = False  # å¯¹å¤–å½“ logits
        return out_bos, out_bom


# import torch
# import torch.nn as nn
# import numpy as np

# class TemperatureCalibrator(nn.Module):
#     """
#     é€šç”¨æ¸©åº¦ç¼©æ”¾æ ¡å‡†å±‚ï¼š
#     - mode='learn'ï¼šå­¦ä¹ ï¼ˆå¯å¾®ï¼‰ï¼ŒT>0 é€šè¿‡ softplus çº¦æŸ
#     - mode='grid' ï¼šç½‘æ ¼æœç´¢ï¼ˆä¸å¯å¾®ï¼‰ï¼Œå†…éƒ¨ç»´æŠ¤å€™é€‰ T åˆ—è¡¨å¹¶é€ä¸ªå–å€¼
#     - æ”¯æŒä¸¤ä¸ªå¤´ï¼ˆbos/bomï¼‰å„è‡ªæ‹¥æœ‰ Tï¼Œä¹Ÿå¯é€‰æ‹©å…±äº«
#     """
#     def __init__(self, model, share_T=False, init_T=1.0, mode='learn', eps=1e-6):
#         super().__init__()
#         self.model = model
#         self.mode = mode
#         self.share_T = share_T
#         self.eps = eps

#         # ä»¥ u å‚æ•°åŒ–ï¼šT = softplus(u) + eps ä»¥ä¿è¯ T>0
#         init_u = np.log(np.exp(init_T - eps) - 1.0) if init_T > eps else 0.0
#         init_u = float(init_u)

#         if share_T:
#             self.u_shared = nn.Parameter(torch.tensor([init_u], dtype=torch.float32))
#             self.u_bos = None
#             self.u_bom = None
#         else:
#             self.u_shared = None
#             self.u_bos = nn.Parameter(torch.tensor([init_u], dtype=torch.float32))
#             self.u_bom = nn.Parameter(torch.tensor([init_u], dtype=torch.float32))

#         # grid æ¨¡å¼ç›¸å…³
#         self._grid_vals_bos = None
#         self._grid_vals_bom = None
#         self._grid_idx = 0

#     def _softplus_T(self, u):
#         return torch.nn.functional.softplus(u) + self.eps  # æ ‡é‡ Tensor

#     def _get_Ts(self):
#         """è¿”å›å½“å‰ bos/bom çš„æ ‡é‡æ¸©åº¦ï¼ˆTensorï¼‰ï¼Œæ ¹æ®æ˜¯å¦å…±äº«å†³å®šã€‚"""
#         if self.share_T:
#             T = self._softplus_T(self.u_shared)
#             return T, T
#         else:
#             T_bos = self._softplus_T(self.u_bos)
#             T_bom = self._softplus_T(self.u_bom)
#             return T_bos, T_bom

#     @torch.no_grad()
#     def compile_model(self, try_script=False):
#         if try_script:
#             try:
#                 self.model = torch.jit.script(self.model)
#             except Exception:
#                 # æŒ‰éœ€é™çº§æˆ–å¿½ç•¥
#                 pass

#     def forward(self, img1, img2):
#         logits_bos, logits_bom = self.model(img1, img2)

#         if self.mode == 'learn':
#             # å¯å¾®å­¦ä¹ ï¼šä½¿ç”¨ softplus(u) çš„å½“å‰å€¼
#             T_bos, T_bom = self._get_Ts()
#         elif self.mode == 'grid':
#             # ç½‘æ ¼æœç´¢ï¼šä»é¢„å…ˆè®¾å¥½çš„å€™é€‰è¡¨é‡Œè¯»å–ï¼ˆå¸¸é‡ï¼Œä¸éœ€è¦æ¢¯åº¦ï¼‰
#             assert (self._grid_vals_bos is not None) and (self._grid_vals_bom is not None), \
#                 "è¯·å…ˆè°ƒç”¨ set_grid(start, stop, num) è®¾å®šå€™é€‰ T"
#             T_bos = torch.tensor([self._grid_vals_bos[self._grid_idx]], 
#                                  dtype=torch.float32, device=logits_bos.device if logits_bos is not None else None)
#             T_bom = torch.tensor([self._grid_vals_bom[self._grid_idx]], 
#                                  dtype=torch.float32, device=logits_bom.device if logits_bom is not None else None)
#         else:
#             raise ValueError(f"Unknown mode: {self.mode}")

#         out_bos = logits_bos / T_bos if logits_bos is not None else None
#         out_bom = logits_bom / T_bom if logits_bom is not None else None
#         return out_bos, out_bom

#     # ====== learn æ¨¡å¼ä¸‹çš„ä¾¿æ·æ–¹æ³• ======
#     def freeze_backbone(self):
#         for p in self.model.parameters():
#             p.requires_grad = False
#         # ä»…æ¸©åº¦ç›¸å…³å‚æ•°å‚ä¸è®­ç»ƒ
#         if self.share_T:
#             self.u_shared.requires_grad = True
#         else:
#             self.u_bos.requires_grad = True
#             self.u_bom.requires_grad = True

#     def temperatures(self):
#         """è¿”å›å½“å‰æ•°å€¼åŒ–åçš„ (T_bos, T_bom)ï¼Œç”¨äºæ‰“å°/è®°å½•ã€‚"""
#         with torch.no_grad():
#             if self.share_T:
#                 T = self._softplus_T(self.u_shared).item()
#                 return T, T
#             else:
#                 return self._softplus_T(self.u_bos).item(), self._softplus_T(self.u_bom).item()

#     # ====== grid æ¨¡å¼ä¸‹çš„ä¾¿æ·æ–¹æ³• ======
#     def set_grid(self, start=0.5, stop=3.0, num=26, share_values=True):
#         """
#         è®¾å®šå€™é€‰ T åˆ—è¡¨ã€‚ä½¿ç”¨ linspace æ›´ç¨³å¦¥ã€‚
#         - share_values=Trueï¼šä¸¤ä¸ªå¤´ç”¨åŒä¸€ç»„å€™é€‰ï¼›å¦åˆ™å¯åˆ†åˆ«è®¾ç½®ã€‚
#         """
#         vals = np.linspace(start, stop, num).astype(np.float32)
#         self._grid_vals_bos = vals
#         self._grid_vals_bom = vals if share_values else vals.copy()
#         self._grid_idx = 0

#     def next_T(self):
#         """
#         è¿›å…¥ä¸‹ä¸€ç»„å€™é€‰ Tï¼ˆä»… grid æ¨¡å¼ä½¿ç”¨ï¼‰ã€‚
#         è¿”å› False è¡¨ç¤ºå·²ç”¨å°½ã€‚
#         """
#         if self.mode != 'grid':
#             return False
#         self._grid_idx += 1
#         if self._grid_vals_bos is None:
#             return False
#         return self._grid_idx < len(self._grid_vals_bos)

#     def grid_len(self):
#         return 0 if self._grid_vals_bos is None else len(self._grid_vals_bos)
